{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #3\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Thursday, Febrary 16th, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Optimization via Descent\n",
    "\n",
    "Suppose you are building a pricing model for laying down telecom cables over a geographical region. Your model takes as input a pair of  coordinates, $(x, y)$, and contains two parameters, $\\lambda_1, \\lambda_2$. Given a coordinate, $(x, y)$, and model parameters, the loss in revenue corresponding to the price model at location $(x, y)$ is described by\n",
    "$$\n",
    "L(x, y, \\lambda_1, \\lambda_2) = 0.000045\\lambda_2^2 y - 0.000098\\lambda_1^2 x - 0.0005\\lambda_1 x\\exp\\left\\{\\left(y^2 - x^2\\right)\\left(\\lambda_1^2 + \\lambda_2^2\\right)\\right\\}\n",
    "$$\n",
    "Read the data contained in `HW3_data.csv`. This is a set of coordinates configured on the curve $y^2 - x^2 = -0.1$. Given the data, find parameters $\\lambda_1, \\lambda_2$ that minimize the net loss over the entire dataset.\n",
    "\n",
    "### Part A\n",
    "- Visually verify that for $\\lambda_1 = 2.05384, \\lambda_2 = 0$, the loss function $L$ is minimized for the given data.\n",
    "- Implement gradient descent for minimizing $L$ for the given data, using the learning rate of 0.001.\n",
    "- Implement stochastic gradient descent for minimizing $L$ for the given data, using the learning rate of 0.001.\n",
    "\n",
    "### Part B\n",
    "- Compare the average time it takes to update the parameter estimation in each iteration of the two implementations. Which method is faster? Briefly explain why this result should be expected.\n",
    "- Compare the number of iterations it takes for each algorithm to obtain an estimate accurate to `1e-3` (you may wish to set a cap for maximum number of iterations). Which method converges to the optimal point in fewer iterations? Briefly explain why this result should be expected.\n",
    "\n",
    "### Part C\n",
    "Compare the performance of stochastic gradient descent for the following learning rates: 1, 0.1, 0.001, 0.0001. Based on your observations, briefly describe the effect of the choice of learning rate on the performance of the algorithm."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
