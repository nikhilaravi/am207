{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Long Homework #3\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Friday, April 14th, 2017 at 11:59pm\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course, we've spent a lot of time learning algorithms for performing inference on complex models and we've spent time using these models to make decisions regarding our data. But in nearly every assignment, the model for the data is specified in the problem statement. In real life, the creative and, arguably, much more difficult task is to start with a broadly defined goal and then to customize or create a model which will meet this goal in some way. In this long homework, we will lead you through the process of model building in simulated real-life conditions. \n",
    "\n",
    "In the dataset called \"sample_reviews\", you'll find a fairly representative selection of Yelp reviews for a (now closed) sushi restaurant called Ino's Sushi in San Francisco. The goal in this assignment is to build a model to help a machine classify any given restaurant (or qualities of a restaurant) as \"good\" or \"bad\" given Yelp reviews. \n",
    "\n",
    "Problem #1 is atypical as it does not involve any programming or (necessarily) difficult mathematics/statistics, however, answering these questions *seriously* will give you a idea of how one might create or select a model for a particular application and your answers will help you with formalizing the model in Problem #2, which is much more technically involved.\n",
    "\n",
    "\n",
    "## Problem #1: Understanding Yelp Review Data As a Human\n",
    "\n",
    "***Grading:*** *We want you to make a genuine effort to mold an ambiguous and broad real-life question into a concrete data science or machine learning problem without the pressure of getting the \"right answer\". As such, we will grade your answer of Problem #1 on a pass/fail basis. Any reasonable answer that demonstrates actual effort will be given a full grade.*\n",
    "\n",
    "Read the reviews and form an opinion regarding the various qualities of Ino's Sushi. Answer the following:\n",
    "\n",
    "- If the task is to summarize the quality of a restaurant in a simple and intuitive way, what might be problemmatic with simply classifying this restaurant as simply \"good\" or \"bad\"? Justify your answers with specific examples from the dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- For Ino's Sushi, categorize the food and the service, separately, as \"good\" or \"bad\" based on all the reviews in the dataset. Be as systematic as you can when you do this.\n",
    "\n",
    "  (**Hint:** Begin by summarizing each review. For each review, summarize the reviewer's opinion on two aspects of the restaurant: food and service. That is, generate a classification (\"good\" or \"bad\") for each aspect based on what the reviewer writes.) \n",
    "  \n",
    "  \n",
    "- Identify statistical weaknesses in breaking each review down into an opinion on the food and an opinion on the service. That is, identify types of reviews that make your method of summarizing the reviewer's opinion on the quality of food and service problematic, if not impossible. Use examples from your dataset to support your argument. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Identify all the ways in which the task in bullet #2 might be difficult for a machine to accomplish. That is, break down the classification task into simple self-contained subtasks and identify how each subtask can be accomplished by a machine (which area of machine learning, e.g. topic modeling, sentiment analysis etc, addressess this type of task).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Describe a complete pipeline for processing and transforming the data to obtain a classification for both food and service for each review. (You are welcome to use our schema in Problem #2 to help you do this)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem #2: Modeling Your Understanding\n",
    "\n",
    "In the dataset \"reviews_processed.csv\", you'll find a database of Yelp reviews for a number of restaurants. These reviews have already been processed and transformed by someone who has completed the (pre) modeling process described in Problem #1. That is, imagine the dataset in \"reviews_processed.csv\" is the result of feeding the raw Yelp reviews through the pipeline someone build for Problem #1.\n",
    "\n",
    "The following is a full list of columns in the dataset and their meanings:\n",
    "\n",
    "I. Relevant to Part A and B:\n",
    "\n",
    "  1. \"review_id\" - the unique identifier for each Yelp review\n",
    "  2. \"topic\" - the subject addressed by the review (0 stands for food and 1 stands for service)\n",
    "  3. \"rid\" - the unique identifier for each restaurant\n",
    "  4. \"count\" - the number of sentences in a particular review on a particular topic\n",
    "  5. \"mean\" - the probability of a sentence in a particular review on a particular topic being positive, averaged over total number of sentences in the review related to that topic.\n",
    "  6. \"var\" - the variance of the probability of a sentence in a particular review on a particular topic being positive, taken over all sentences in the review related to that topic.\n",
    "  7. (only relevant\n",
    "\n",
    "II. Relevant (possibly) to Extra Credit:\n",
    "\n",
    "  1. \"uavg\" - the average star rating given by a particular reviewer (taken across all their reviews)\n",
    "  2. \"stars\" - the number of stars given in a particular review\n",
    "  3. \"max\" - the max probability of a sentence in a particular review on a particular topic being positive\n",
    "  4. \"min\" - the min probability of a sentence in a particular review on a particular topic being positive\n",
    "\n",
    "The following schema illustrates the model of the raw data that is used to generate \"reviews_processed.csv\":\n",
    "<img src=\"restuarant_model.pdf\">\n",
    "\n",
    "***Warning:*** *this is a \"real\" data science problem in the sense that the dataset in \"reviews_processed.csv\" is large. We understand that a number of you have limited computing resources, so you are encouraged but not required to use the entire dataset. If you wish you may use 10 restaurants from the dataset, as long as your choice of 10 contains a couple of restaurants with a large number of reviews and a couple with a small number of reviews.*\n",
    "\n",
    "### Part A: Modeling\n",
    "\n",
    "When the value in \"count\" is low, the \"mean\" value can be very skewed (refer to your answers for Problem #1 to see why this is a problem if we are interested in summarizing the reviewer's opinion on each aspect of a restaurant).\n",
    "\n",
    "Following the [SAT prep school example discussed in lab](https://am207.github.io/2017/wiki/gelmanschoolstheory.html) (and using your answers for Problem #1), set up a Bayesian model for a reviewer $j$'s opinion of restaurant $k$'s food and service, separately. That is, you will have a model for each restaurant and each aspect (food and serivce). For restaurant $k$, you will have a model for $\\{\\theta_{jk}^{\\text{food}}\\}$ and one for $\\{\\theta_{jk}^{\\text{service}}\\}$, where $\\theta_{jk}$ is the positivity of the opinion of the $j$-th reviewer regarding the $k$-th restaurant. \n",
    "\n",
    "**Hint:** what quantity in our data naturally corresponds to $\\bar{y}_j$'s in the prep school example? How would you calculate the parameter $\\sigma_j^2$ in the distribution of $\\bar{y}_j$ (note that, contrary to the school example, $\\sigma_j^2$ is not provided explictly in the restaurant data)?\n",
    "\n",
    "### Part B: Analysis for Each restaurant\n",
    "\n",
    "Use your model to produce estimates for $\\theta_{jk}$'s. Pick a few restaurants, for each aspect (\"food\" and \"service\") of each restaurant, plot your estimates for the $\\theta$'s against the values in the \"mean\" column (corresponding to this restaurant. \n",
    "\n",
    "For the same restaurants, for each aspect, generate shrinkage plots as follows:\n",
    "\n",
    "<img src=\"./shrinkage.png\">\n",
    "\n",
    "The $x$-axis is the posterior means, the $y$-axis is classification probability (1-cdf) or fraction of predictive samples. The colored lines are error bars. (The code to generate this plot is included in this notebook.)\n",
    "\n",
    "Use these plots to discuss the statistical benefits of modeling each reviewer's opinion as you did in Part A, rather than approximating the reviewer opinion with the value in \"mean\".\n",
    "\n",
    "### Part C: Analysis Across Restaurants\n",
    "\n",
    "Aggregate, in a simple but reasonable way, the reviewer's opinions to given a pair of overall scores for each restaurant, one for food and one for service. Rank the restaurants by food score and then by service score. Discuss the statistical weakness of ranking by these scores.\n",
    "\n",
    "(**Hint:** what is statistically problemmatic about the way you aggregated the reviews of each restaurant to produce an overall food or service score? You've see this question addressed a number of times in previous homeworks, e.g. Homework #7 and Homework #6. This is also the same problem with summarizing a reviewer's opinion on a restaurants service and food based on what they write.)\n",
    "\n",
    "### Extra Credit:\n",
    "\n",
    "Propose a model, that addresses the weakness of your approach in Part C, for the overall quality of food and service for each restaurant given the $\\theta$'s. Combine your model for the overall quality with your model for the $\\theta$'s. Use this combined model to estimate the overall quality of food and service for each restaurant.\n",
    "\n",
    "(**Hint:** Homework #7 is a good reference for building your model for the overall quality of food and service of these restaurants. Specifically, thinking about pooling.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# fix a restaurant and an aspect (food or service)\n",
    "# \"means\" is the array of values in the \"mean\" column for the restaurant and the aspect \n",
    "#         in the dataset\n",
    "# \"thetas\" is the array of values representing your estimate of the opinions of reviewers \n",
    "#          regarding this aspect of this particular restaurant\n",
    "# \"theta_vars\" is the array of values of the varaiances of the thetas\n",
    "# \"counts\" is the array of values in the \"count\" column for the restaurant and the aspect \n",
    "#.         in the dataset\n",
    "\n",
    "def prob_shrinkage_plot(means, thetas, theta_vars, counts):\n",
    "    data = zip(means, thetas, theta_vars / counts, theta_vars, counts)\n",
    "    palette = itertools.cycle(sns.color_palette())\n",
    "    with sns.axes_style('white'):\n",
    "        for m,t, me2, te2, c in data:\n",
    "            color = next(palette)\n",
    "            noise = 0.001 * np.random.randn()\n",
    "            noise2 = 0.001 * np.random.randn()\n",
    "            if me2 == 0:\n",
    "                me2 = 4\n",
    "            p = prob(m, me2, 1.)\n",
    "            peb = prob(t, te2, 1.)\n",
    "            plt.plot([m, t],[p, peb],'o-', color=color, lw=1)\n",
    "            plt.errorbar([m, t],[p + noise, peb + noise2], xerr=[np.sqrt(me2), np.sqrt(te2)], color=color, lw=1)\n",
    "        ax = plt.gca()\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1.05])\n",
    "    return ax"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
